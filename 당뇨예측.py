# -*- coding: utf-8 -*-
"""당뇨예측.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qu9lrgU24LwRLSdaFWjM3oBSuwZ0u6HD
"""

import matplotlib
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.models import load_model
from keras.layers import Dense
import matplotlib.pyplot as plt
import numpy as np
np.random.seed(16)

from google.colab import drive
drive.mount('/content/drive')

filename = '/content/drive/MyDrive/기계학습/Data/diabetes.csv'
df = pd.read_csv(filename)
print(df)

# 상위 5개 데이터를 출력한다
df.head(5)

# 히스토그램을 살펴본다
df.hist()
plt.tight_layout()
plt.show()

df.info()

df.describe() # 딥러닝은 정규화의 큰 영향을 받지 않음

df.corr() # 다중공성선은 확인 필요

# 데이터 간의 상관 관계를 그래프로 표현해 봅니다.
colormap = plt.cm.gist_heat   # 그래프의 색상 구성을 정합니다.
plt.figure(figsize=(12,12))   # 그래프의 크기를 정합니다.

# 그래프의 속성을 결정합니다. vmax의 값을 0.5로 지정해 0.5에 가까울수록 밝은색으로 표시되게 합니다.
sns.heatmap(df.corr(),linewidths=0.1,vmax=0.5, cmap=colormap, linecolor='white', annot=True)
plt.show()

import warnings
warnings.filterwarnings("ignore")

# plasma를 기준으로 각각 정상과 당뇨가 어느 정도 비율로 분포하는지 살펴봅니다.
# 데이터 불균형이 심해보이지 않음
plt.hist(x=[df.Glucose[df.Outcome==0], df.Glucose[df.Outcome==1]], bins=30, histtype='barstacked', label=['normal','diabetes'])
plt.legend()

# BMI를 기준으로 각각 정상과 당뇨가 어느 정도 비율로 분포하는지 살펴봅니다.
plt.hist(x=[df.BMI[df.Outcome==0], df.BMI[df.Outcome==1]], bins=30, histtype='barstacked', label=['normal','diabetes'])
plt.legend()

scaler = MinMaxScaler()
df[:] = scaler.fit_transform(df[:])

# 데이터를 훈련 데이터셋과 테스트 데이터셋으로 분할
X = df.loc[:, df.columns != 'Outcome']
y = df.loc[:, 'Outcome']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2024)

# 케라스 신경망을 구축(구조)
model = Sequential() # 심층신경망 쌓으려면 필요
model.add(Dense(16, activation='relu', input_dim=8)) # 첫번째 은닉층, 은닉노드 16개, activation: 활성함수(ReLU), input_dim: 입력 노드(독립 변수) 수
# Dense: 다음 노드로 정보 빠짐없이 모두 전달
# 그냥 계산하면 숫자가 점점 커지기 때문에 활성함수를 사용한다.
model.add(Dense(8, activation='relu')) # 두번쨰 은닉층, 은닉노드 8개
model.add(Dense(1, activation='sigmoid')) # 출력층, 출력 활성함수로 시그모이드 사용
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # model.compile: 학습, optimizer: 학습률 보정, loss: 목적함수(손실함수)
# 이진분류: binary_crossentropy, 예측: mse, rmse
model.fit(X_train, y_train, epochs=50, batch_size=20) # model.fit: 학습, epochs: 학습 횟수, batch_size: 한 번 학습에 랜덤으로 20명 추출

# 결과 - 정확도
scores = model.evaluate(X_train, y_train, verbose=False) # model.evaluate: 정확도 계산
print("Training Accuracy: %.2f%%\n" % (scores[1]*100))
scores = model.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy: %.2f%%\n" % (scores[1]*100))

# 케라스 신경망을 구축
model1 = Sequential()
model1.add(Dense(32, activation='relu', input_dim=8))
model1.add(Dense(16, activation='relu'))
model1.add(Dense(8, activation='relu'))
model1.add(Dense(1, activation='sigmoid'))
model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model1.fit(X_train, y_train, epochs=50, batch_size=20, validation_split=0.2)

# 결과 - 정확도
scores = model1.evaluate(X_train, y_train, verbose=False) # model.evaluate: 정확도 계산
print("Training Accuracy: %.2f%%\n" % (scores[1]*100))
scores = model1.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy: %.2f%%\n" % (scores[1]*100))

# 케라스 신경망을 구축
model2 = Sequential()
model2.add(Dense(32, activation='relu', input_dim=8))
model2.add(Dense(16, activation='relu'))
model2.add(Dense(16, activation='relu'))
model2.add(Dense(8, activation='relu'))
model2.add(Dense(1, activation='sigmoid'))
model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model2.fit(X_train, y_train, epochs=50, batch_size=20, validation_split=0.2)

# 결과 - 정확도
scores = model2.evaluate(X_train, y_train, verbose=False) # model.evaluate: 정확도 계산
print("Training Accuracy: %.2f%%\n" % (scores[1]*100))
scores = model2.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy: %.2f%%\n" % (scores[1]*100))

# 케라스 신경망을 구축
model3 = Sequential()
model3.add(Dense(64, activation='relu', input_dim=8))
model3.add(Dense(32, activation='relu'))
model3.add(Dense(16, activation='relu'))
model3.add(Dense(8, activation='relu'))
model3.add(Dense(1, activation='sigmoid'))
model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model3.fit(X_train, y_train, epochs=50, batch_size=20, validation_split=0.2)

# 결과 - 정확도
scores = model3.evaluate(X_train, y_train, verbose=False) # model.evaluate: 정확도 계산
print("Training Accuracy: %.2f%%\n" % (scores[1]*100))
scores = model3.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy: %.2f%%\n" % (scores[1]*100))

# 케라스 신경망을 구축
model4 = Sequential()
model4.add(Dense(64, activation='relu', input_dim=8))
model4.add(Dense(32, activation='relu'))
model4.add(Dense(32, activation='relu'))
model4.add(Dense(16, activation='relu'))
model4.add(Dense(8, activation='relu'))
model4.add(Dense(1, activation='sigmoid'))
model4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model4.fit(X_train, y_train, epochs=50, batch_size=20, validation_split=0.2)

# 결과 - 정확도
scores = model4.evaluate(X_train, y_train, verbose=False) # model.evaluate: 정확도 계산
print("Training Accuracy: %.2f%%\n" % (scores[1]*100))
scores = model4.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy: %.2f%%\n" % (scores[1]*100))

# 케라스 신경망을 구축
model5 = Sequential()
model5.add(Dense(64, activation='relu', input_dim=8))
model5.add(Dense(64, activation='relu'))
model5.add(Dense(32, activation='relu'))
model5.add(Dense(32, activation='relu'))
model5.add(Dense(16, activation='relu'))
model5.add(Dense(8, activation='relu'))
model5.add(Dense(1, activation='sigmoid'))
model5.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model5.fit(X_train, y_train, epochs=50, batch_size=20, validation_split=0.2)

# 결과 - 정확도
scores = model5.evaluate(X_train, y_train, verbose=False) # model.evaluate: 정확도 계산
print("Training Accuracy: %.2f%%\n" % (scores[1]*100))
scores = model5.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy: %.2f%%\n" % (scores[1]*100))

# 케라스 신경망을 구축
model6 = Sequential()
model6.add(Dense(128, activation='relu', input_dim=8))
model6.add(Dense(64, activation='relu'))
model6.add(Dense(32, activation='relu'))
model6.add(Dense(32, activation='relu'))
model6.add(Dense(16, activation='relu'))
model6.add(Dense(16, activation='relu'))
model6.add(Dense(8, activation='relu'))
model6.add(Dense(1, activation='sigmoid'))
model6.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model6.fit(X_train, y_train, epochs=50, batch_size=20, validation_split=0.2)

# 결과 - 정확도
scores = model6.evaluate(X_train, y_train, verbose=False) # model.evaluate: 정확도 계산
print("Training Accuracy: %.2f%%\n" % (scores[1]*100))
scores = model6.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy: %.2f%%\n" % (scores[1]*100))

# 케라스 신경망을 구축
model7 = Sequential()
model7.add(Dense(64, activation='relu', input_dim=8))
model7.add(Dense(32, activation='relu'))
model7.add(Dense(32, activation='relu'))
model7.add(Dense(32, activation='relu'))
model7.add(Dense(16, activation='relu'))
model7.add(Dense(8, activation='relu'))
model7.add(Dense(1, activation='sigmoid'))
model7.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model7.fit(X_train, y_train, epochs=50, batch_size=20, validation_split=0.2)

# 결과 - 정확도
scores = model7.evaluate(X_train, y_train, verbose=False) # model.evaluate: 정확도 계산
print("Training Accuracy: %.2f%%\n" % (scores[1]*100))
scores = model7.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy: %.2f%%\n" % (scores[1]*100))

# 케라스 신경망을 구축
model8 = Sequential()
model8.add(Dense(64, activation='relu', input_dim=8))
model8.add(Dense(32, activation='relu'))
model8.add(Dense(32, activation='relu'))
model8.add(Dense(16, activation='relu'))
model8.add(Dense(8, activation='relu'))
model8.add(Dense(8, activation='relu'))
model8.add(Dense(1, activation='sigmoid'))
model8.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model8.fit(X_train, y_train, epochs=50, batch_size=20, validation_split=0.2)

# 결과 - 정확도
scores = model8.evaluate(X_train, y_train, verbose=False) # model.evaluate: 정확도 계산
print("Training Accuracy: %.2f%%\n" % (scores[1]*100))
scores = model8.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy: %.2f%%\n" % (scores[1]*100))

# 케라스 신경망을 구축
model9 = Sequential()
model9.add(Dense(64, activation='relu', input_dim=8))
model9.add(Dense(32, activation='relu'))
model9.add(Dense(32, activation='relu'))
model9.add(Dense(32, activation='relu'))
model9.add(Dense(16, activation='relu'))
model9.add(Dense(8, activation='relu'))
model9.add(Dense(1, activation='sigmoid'))
model9.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model9.fit(X_train, y_train, epochs=50, batch_size=20, validation_split=0.2)

# 결과 - 정확도
scores = model9.evaluate(X_train, y_train, verbose=False) # model.evaluate: 정확도 계산
print("Training Accuracy: %.2f%%\n" % (scores[1]*100))
scores = model9.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy: %.2f%%\n" % (scores[1]*100))

# 케라스 신경망을 구축
model10 = Sequential()
model10.add(Dense(64, activation='relu', input_dim=8))
model10.add(Dense(32, activation='relu'))
model10.add(Dense(32, activation='relu'))
model10.add(Dense(16, activation='relu'))
model10.add(Dense(8, activation='relu'))
model10.add(Dense(1, activation='sigmoid'))
model10.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model10.fit(X_train, y_train, epochs=50, batch_size=20, validation_split=0.2)

# 결과 - 정확도
scores = model10.evaluate(X_train, y_train, verbose=False) # model.evaluate: 정확도 계산
print("Training Accuracy: %.2f%%\n" % (scores[1]*100))
scores = model10.evaluate(X_test, y_test, verbose=False)
print("Testing Accuracy: %.2f%%\n" % (scores[1]*100))